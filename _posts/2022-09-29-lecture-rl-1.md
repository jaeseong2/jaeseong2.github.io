---
title: DeepMind x UCL RL Lecture Series - 1. Introduction to Reinforcement Learning
date: 2022-09-29 00:09:00 +0900
categories: [Lectures]
tags: [ml, rl]
---

예전에 오목 프로젝트를 진행할 때 [RL Course by David Silver](https://www.davidsilver.uk/teaching/)를 들었었다. 시간이 좀 지나기도 했고 그때 완벽하게 이해를 하지 못했어서 강의를 다시 들어보기로 했다. 강의를 찾던 중 작년에 딥마인드에서 진행한 새로운 강의를 알게 되었다. 이전의 강의보다 음질과 영상에서 훨씬 낫기도 하고 더 최근의 강의이기 때문에 이걸 듣기로 했다.

# Introduction to Reinforcement Learning

- [Video](https://www.youtube.com/watch?v=TCCjZe0y4Qc&list=PLqYmG7hTraZDVH599EItlEWsUOsJbAodm&index=1&ab_channel=DeepMind)
- [Slides](https://storage.googleapis.com/deepmind-media/UCL%20x%20DeepMind%202021/Lecture%201%20-%20introduction.pdf)


## What is reinforcement learning?

We can learn to get a goal whiout examples of optimal behavior, through repeated interaction.
There are distinct reasons to learn:
1. Find solutions
2. Adapt online, deal with unforeseen circumstances

강화학습으로는 해결법을 찾을 수 있을 뿐만 아니라 계속 변화하는 상황에 대한 해결책 역시도 찾을 수 있다.

## Agent, Environment, Rewards, Values

매 스텝마다 에이전트는 관측 $O_t$과 reward $R_t$를 얻고 다음 action $A_t$을 취한다.
그럼 환경은 action $A_t$을 받아 새로운 관측 $O_{t+1}$과 새로운 reward $R_{t+1}$를 얻는다.

여기서 reward는 액션에 대한 스칼라 피드백이다. 우리는 reward의 누적합 $G_t$이 최대가 되면 목표에 도달할 수 있다고 가정하기 때문에 에이전트는 $G_t$을 최대로 하려한다.

Value는 위에서 언급한 reward 누적합의 기댓값이다. 다음과 같이 재귀적으로 정의할 수도 있다.
```math
v(s) = E[G_t | S_t=s]
G_t = R_{t+1} + G_{t+1}
v(s) = E[R_{t+1} + v(S_{t+1}) | S_t=s]
```

`Goal: select actions to maximize value`

## Agent components

### Agent state






