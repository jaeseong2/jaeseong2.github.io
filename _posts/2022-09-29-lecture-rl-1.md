---
title: DeepMind x UCL RL Lecture Series - 1. Introduction to Reinforcement Learning
date: 2022-09-29 00:09:00 +0900
categories: [Lectures, RL]
tags: [ml, rl]
math: true
---

예전에 오목 프로젝트를 진행할 때 [RL Course by David Silver](https://www.davidsilver.uk/teaching/)를 들었었다. 시간이 좀 지나기도 했고 그때 완벽하게 이해를 하지 못했어서 강의를 다시 들어보기로 했다. 강의를 찾던 중 작년에 딥마인드에서 진행한 새로운 강의를 알게 되었다. 이전의 강의보다 음질과 영상에서 훨씬 낫기도 하고 더 최근의 강의이기 때문에 이걸 듣기로 했다.

# Introduction to Reinforcement Learning

- [Video](https://www.youtube.com/watch?v=TCCjZe0y4Qc&list=PLqYmG7hTraZDVH599EItlEWsUOsJbAodm&index=1&ab_channel=DeepMind)
- [Slides](https://storage.googleapis.com/deepmind-media/UCL%20x%20DeepMind%202021/Lecture%201%20-%20introduction.pdf)


## What is reinforcement learning?

We can learn to get a goal whiout examples of optimal behavior, through repeated interaction.
There are distinct reasons to learn:
1. Find solutions
2. Adapt online, deal with unforeseen circumstances

강화학습으로는 해결법을 찾을 수 있을 뿐만 아니라 계속 변화하는 상황에 대한 해결책 역시도 찾을 수 있다.

## Agent, Environment, Rewards, Values

매 스텝마다 에이전트는 관측 $O_t$과 reward $R_t$를 얻고 다음 action $A_t$을 취한다.
그럼 환경은 action $A_t$을 받아 새로운 관측 $O_{t+1}$과 새로운 reward $R_{t+1}$를 얻는다.

여기서 reward는 액션에 대한 스칼라 피드백이다. 우리는 reward의 누적합 $G_t$이 최대가 되면 목표에 도달할 수 있다고 가정하기 때문에 에이전트는 $G_t$을 최대로 하려한다.

Value는 위에서 언급한 reward 누적합의 기댓값이다. 다음과 같이 재귀적으로 정의할 수도 있다.

$$\begin{gathered}
v(s) = \mathbb{E}[G_t | S_t=s] \\\\
G_t = R_{t+1} + G_{t+1} \\\\
v(s) = \mathbb{E}[R_{t+1} + v(S_{t+1}) | S_t=s]
\end{gathered}$$

`Goal: select actions to maximize value`

## Agent components

### Agent state
History는 observation, action, reward의 배열이다.
Agent state $S_t$는 history로 구성된다.
여기서 Markov decision processes라는 중요한 개념이 나온다.

$$\begin{aligned}
p(r, s | S_t, A_t) = p(r, s | H_t, A_t)
\end{aligned}$$

Agent state에 우리에게 필요한 정보가 모두 담겨있고, $S_t$로 $H_t$를 대신할 수 있기 때문에 위에서 언급한 history를 모두 저장하지 않아도 된다는 뜻이다.

partial observability은 observation이 Markovian이 아니라는 의미이고, state 역시 Markovian이 아니다.
이런 경우, partially observable Markov decision process (POMDP) 라고 한다.

다음과 같은 예시에서 두 상태를 관측만으로는 구분할 수 없다.

![partially observation](/assets/images/partially_observation.PNG)

따라서 partially observable environment에서 agent는 일반적으로 다음과 같이 state를 갱신한다.

$$
S_t = u(S_{t-1}, A_{t-1}, R_t, O_t)
$$

하지만 u라는 함수를 어떻게 찾는가 역시 문제이다. 그래서 완전하게 Markov한 state를 만드는건 매우 힘들다고 한다.
그런 Markov한 state를 만드는 것보다 더 중요한건 좋은 policy와 value function을 찾아낼 수 있는 state이다.

### Policy

policy는 state를 입력으로 하고 action을 출력으로 하는 매핑이다.

Deterministic policy: $A = \pi(S)$
Stochastic policy: $\pi(A|S) = p(A|S)$

### Value functions

$$\begin{aligned}
v(s) & = \mathbb{E}[G_t | S_t=s, \pi] \\
& = \mathbb{E}[R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ... | S_t=s, \pi]
\end{aligned}$$

discount factor $\gamma$ 란 더 후에 받을 것으로 예상되는 리워드에 곱해주는 계수이다. discount factor는 더 큰 reward에 도달하기 위한 step을 줄여주는 역할을 한다. 하지만, 0에 근접하게 된다면 long term reward를 모두 잃을 수도 있기 때문에 적절한 값을 선택하는 것이 중요하다.

위의 return에도 동시에 discoutn factor를 적용할 수 있다.

$$
G_t = R_{t+1} + \gamma G_{t+1}
$$

그럼 value는 다음과 같이 나타낼 수 있다.

$$\begin{aligned}
v_{\pi}(s) 
& = \mathbb{E}[R_{t+1} + \gamma G_{t+1} | S_t=s, A_t \sim \pi(s)] \\
& = \mathbb{E}[R_{t+1} + \gamma v_{\pi}(S_{t+1}) | S_t=s, A_t \sim \pi(s)]
\end{aligned}$$

이게 Bellman equation이다.

optimal value에 대해 유사한 식이 있다.

$$
v_{\pi}(s) = \max_a \mathbb{E}[R_{t+1} + \gamma v_{\pi}(S_{t+1}) | S_t=s, A_t=a]
$$

이 식은 policy에 종속하지 않는다. 그래서 state를 통한 value function만의 계산으로도 목표에 도달할 수 있다.

### Model

이 부분에서 조금 이해가 안됐었는데 그 이유는 딥러닝에서의 모델과 강화학습에서의 모델이 다르기 때문이다. 강화학습에서는 환경에 대한 가정을 모델이라고 한다. 다음 state와 다음 reward를 예측하는 모델이 있다.

## Agent Categories

- Value Based

    value function이 완벽하다면 가장 높은 value를 주는 state를 따라가면 된다. 이를 implicit policy라고 한다.

- Policy Based

    반대로 policy가 완벽하다면 value를 계산할 필요가 없다. 그래서 value function 없이 policy만을 학습하는 agent이다.

- Actor Critic

    value funciton과 policy 모두를 학습하는 agent이다.

- Model Free

    앞에서 언급했듯이 model은 환경에 대한 예측이다. 환경에 대한 예측없이 (환경에 대한 사전지식이 없는 등의 이유로) 학습을 진행하는 방식이다. 

- Model Based

    환경에 대한 기존 지식을 활용하거나 학습해서 사용하는 방식이다. 내가 지금 만들고 있는 오목 AI도 모델 베이스이다.
