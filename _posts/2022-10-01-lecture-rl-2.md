---
title: DeepMind x UCL RL Lecture Series - 2. Exploration & Control
date: 2022-10-01  22:57:00 +0900
categories: [Lectures]
tags: [ml, rl, policy gradient, ucb, ]
math: true
---

# Exploration & Control

- [Video](https://www.youtube.com/watch?v=aQJP3Z2Ho8U&list=PLqYmG7hTraZDVH599EItlEWsUOsJbAodm&index=2&ab_channel=DeepMind)
- [Slides](https://storage.googleapis.com/deepmind-media/UCL%20x%20DeepMind%202021/Lecture%202-%20Exploration%20and%20control_slides.pdf)


## Exploration vs. Exploitation

exploitation은 기존에 우리가 알고 있는 지식들로만 최대의 reward를 가질 수 있도록 하는 것이다. 반대로, exploration은 새로운 시도를 통해 지식을 늘리는 것이다.
exploitation만 하게 된다면, 즉 greedy하게 action을 취한다면 local optimal에 갇힐 수 있다. 그래서 exploration을 통해 새로운 지식을 추가해야 한다. 그렇다면 이 exploration에 대한 확률을 어떻게 가져가면 좋을까?

## Values and Regret

위의 exploration 알고리즘에 대해 소개하기 이전에 value와 regret에 대해 먼저 설명을 하신다.

### Action value

action value for action $a$ is the expected reward.

$$
q(a) = \mathbb{E}[R_t \ |\ A_t = a]
$$

$q(a)$는 실제의 그것이고, $Q_t(a)$는 우리가 추정하는 값이다.
간단하게 다음과 같이 추정할 수 있다.

$$
Q_t(a)\ =\ {\sum_{n=1}^t\ I(A_n = a)\ R_n \over \sum_{n=1}^t I(A_n = a)}
$$

$I$는 indicator function으로 True일 때 1이고, False일 때 0이다.
위의 평균 추정식에서 분모가 되는 걸 action에 대한 count $N_t(a)$라고 한다.

추정값 $Q_t(a)$는 다음과 같이 업데이트될 수 있다.

$$\begin{align}
Q_t(A_t)\ & =\ Q_{t-1}(A_t)\ +\ \alpha_t(R_t\ - Q_{t-1}(A_t)) \\
\forall\ a\ \neq\ A_t\ :\ Q_t(a)\ & =\ Q_{t-1}(a) \\\\

with\quad \alpha_t\ =\ {1 \over N_t(A_t)}\quad & and \quad N_t(A_t)\ =\ N_{t-1}(A_t)\ +\ 1
\end{align}$$

step size $\alpha$는 단순히 이렇게 카운트로 나눠주기 보다는 tracking 같은 방법도 있다고 한다.

### Regret

optimal value $v_*$는 당연하게 optimal action에 대한 q(a)이고 여기서 regret $\Delta_a$은 $v_*\ - \ q(a)$이다.
total regret $L_t$는 다음과 같다.

$$
L_t\ = \sum_{n=1}^t\ v_* - q(A_n)\ = \sum_{n=1}^t\ \Delta_{A_n}
$$

cumulative reward를 최대로 하는 것은 위의 total regret을 최소화하는 것과 같다.


## Greedy Algorithm

가장 간단한 policy 중 하나로 매번 $Q_t(a)$를 최대로 만드는 $a$를 선택하는 것이다.
하지만 위에서 언급했듯이 local optimal에 갇힐 수 있다.

## $\epsilon$-Greedy Algorithm

적당히 작은 $\epsilon$을 정해서 다음과 같이 랜덤하게 액션을 하는 것이다.

$$
\pi_t(a)\ =\ 
\begin{cases}
\ (1-\epsilon)\ +\ \epsilon/|\mathbb{A}| & \text{if}\quad Q_t(a)\ = \max_b\ Q_t(b) \\
\ \epsilon/|\mathbb{A}| & \text{otherwise}
\end{cases}
$$

하지만 계속해서 exploration을 한다는 문제가 있다.

근데 그러면 decaying $\epsilon$을 사용하면 되지 않을까? 

## Policy gradients

value를 학습하는 것 대신 policy만을 직접적으로 학습하는 방법이다.
예시로, action preferences $H_t$라는 policy parameter가 있을 때 softmax 함수를 이용해서 다음과 같이 policy를 정의할 수 있다.

$$
\pi(a)\ =\ {e^{H_t(a)} \over \sum_b e^{H_t(b)}} 
$$

이 때 action prefereces가 커지도록 gradient ascent로 policy parameter를 업데이트해 줄 수 있다.

$\theta_t$가 현재의 policy parameters일 때, 다음과 같은 식으로 업데이트할 수 있다.

$$
\theta_{t+1}\ =\ \theta_t\ +\ \alpha \nabla_\theta \mathbb{E}[R_t|\pi_{\theta_t}]
$$

하지만 여기서 기댓값을 계산하기 힘들기 때문에 gradient를 구할 수 없다. 그래서 여기서 수학적인 트릭을 이용한다. \log-likelihood 트릭을 이용하면 다음과 같이 term을 바꿀 수 있다. 과정을 모두 latex로 정리하기는 귀찮아서 스킵한다. [여기](http://stillbreeze.github.io/REINFORCE-vs-Reparameterization-trick/)서 볼 수 있다. 결과는 다음과 같다.

$$
\nabla_\theta \mathbb{E}[R_t|\theta] = \mathbb{E}[R_t\nabla_\theta \log\pi_\theta(A_t)] \\
\theta =\ \theta\ +\ \alpha R_t\nabla_\theta \log\pi_\theta(A_t)
$$

이제 단순히 샘플된 action과 reward로 $\theta$를 업데이트할 수 있다.

이를 그대로 위의 softmax 함수에 적용하면 다음과 같이 나타낼 수 있다. softmax의 미분은 [여기](https://towardsdatascience.com/derivative-of-the-softmax-function-and-the-categorical-cross-entropy-loss-ffceefc081d1)서 볼 수 있다.

$$\begin{aligned}
H_{t+1}(A_t)\ & =\ H_t(A_t)\ +\ \alpha R_t(1\ -\ \pi_t(A_t))  \\
H_{t+1}(a)\ & =\ H_t(A_t)\ -\ \alpha R_t\pi_t(a)\qquad \text{if}\ a \neq A_t
\end{aligned}$$

샘플된 action에 대한 reward가 양수라면 $\pi_t(A_t)$는 1보다 작기 때문에 샘플된 액션에 대한 preference는 증가하고, 나머지 액션에 대한 preference는 감소한다. 음수라면 그 반대이다. 딥러닝을 적용할 수 있는 방법이지만 suboptimal에 갇힐 수 있다.

같이 baseline에 대한 설명도 하셨는데, 이는 다른 강의에서 다룬다고 하신다.

### Lai and Robbins

$$
\lim_{t\rightarrow\infty}\ L_t \ge\ \log\ t \sum_{a|\Delta_a \gt 0} {\Delta_a \over KL(\mathbb{R}_a||\mathbb{R}_{a_*})}
$$

KL divergence가 regret의 제곱에 비례한다. 그래서 regret은 log만큼 커진다.


## Upper Confidence Bounds

$q(a) \le Q_t(a)\ +\ U_t(a)$를 높은 확률로 만족하는 upper confidence $U_t(a)$가 각 action마다 추정할 수 있다. 그럴 때 upper confidence bound가 최대가 되는 액션을 고르는 알고리즘이다.

$$
a_t\ =\ \argmax_{a \in \mathbb{A}} Q_t(a) + U_t(a)
$$

이 때 upper confidence는 uncertainty가 높을수록, 즉 count가 작을수록 높아야 하고, (그래야 exploration을 하니까) 이미 count가 충분하다면 정확도를 위해 작아야한다. 이럴 때 어떻게 적절한 upper confidence bound를 유도할 수 있을까?

### Hoeffding's Inequality

$X_1, ..., X_n$을 0과 1사이의 구간에서 무작위로 뽑은 값이라고 할 때, 실제 평균 $\mu\ =\ \mathbb{E}[X]$과 표본 평균 $\overline{X_t}$ 사이에는 다음과 같은 부등식을 만족한다.

$$
p(\overline{X_t}\ +\ u\ \le\ \mu)\ \le e^{-2nu^2}
$$

이 호에프딩 부등식을 위의 upper bound에 적용할수 있다. $n$은 count, $u$는 upper confidence가 된다. 이 때 p는 우리가 관측을 진행하면 할 수록 줄어들기 때문에 t에 따라 감소하는 함수가 될 수 있다. 여기에서는 예시로 $1/t$을 들었다. 그렇게 정리를 하면 다음과 같이 upper confidence를 구할 수 있다.

$$
U_t(a) = \sqrt{\log{t} \over 2N_t(a)}
$$

이 식은 exploration을 계속해야 하면서도 그렇게 많이는 하면 안된다는 것을 말해준다.

그리고 p가 $1/t^{-4}$일 때 위에서 언급한 Lai and Robbins regret lower bound를 만족한다. 그 유도 과정도 소개해주시지만 정리는 생략한다..

## Bayesian

강의에서 예시로 든 상황인 bandits에서 bayesian으로 접근할 수도 있다. policy parameters에 대한 사전지식을 piror를 통해 적용할 수 있고, posterior를 exploration을 위한 지표로 사용할 수 있다.

### Bayesian with UCB

![bayesian_with_UCB](/assets/images/bayesian_with_ucb.PNG)
upper confidence bound를 posterior에서 추정할 수 있다. 위와 같은 가우시안 분포에서 $U_t(a) = c\sigma_t(a)$라고 한다면, $Q_t(a)\ +\ c\sigma(a)$가 최대가 되는 aciton을 고르면 된다.

## Thompson Sampling

### Probability Matching

UCB와 다르게 확률적인 관점에서 $q(a)$가 최대일 수 있는 확률에 따라 action을 뽑는 것이다. 하지만 위 확률을 계산하는 건 매우 어렵다.

### Thompson Sampling

그래서 대신에 사후확률 분포를 이용해서 샘플링한 결과를 이용하자는 것이 Thompson Sampling이다. 타임스텝 T에서의 사후확률 분포에서 모든 action에 대해 샘플을 뽑고 그 중 가장 큰 Q(a)를 가지는 action을 선택하는 것이다.

이렇게 샘플링한 결과를 이용하더라도 베르누이 bandits에 대해서 Thompson sampling은 Lai and Robbins regret lower bound를 만족한다고 한다.
