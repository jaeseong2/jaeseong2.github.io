---
title: DeepMind x UCL RL Lecture Series - 2. Exploration & Control
date: 2022-10-01  22:57:00 +0900
categories: [Lectures]
tags: [ml, rl]
math: true
---

# Exploration & Control

- [Video](https://www.youtube.com/watch?v=aQJP3Z2Ho8U&list=PLqYmG7hTraZDVH599EItlEWsUOsJbAodm&index=2&ab_channel=DeepMind)
- [Slides](https://storage.googleapis.com/deepmind-media/UCL%20x%20DeepMind%202021/Lecture%202-%20Exploration%20and%20control_slides.pdf)


## Exploration vs. Exploitation

exploitation은 기존에 우리가 알고 있는 지식들로만 최대의 reward를 가질 수 있도록 하는 것이다. 반대로, exploration은 새로운 시도를 통해 지식을 늘리는 것이다.
exploitation만 하게 된다면, 즉 greedy하게 action을 취한다면 local optimal에 갇힐 수 있다. 그래서 exploration을 통해 새로운 지식을 추가해야 한다. 그렇다면 이 exploration에 대한 확률을 어떻게 가져가면 좋을까?

## Values and Regret

위의 exploration 알고리즘에 대해 소개하기 이전에 value와 regret에 대해 먼저 설명을 하신다.

### Action value

action value for action $a$ is the expected reward.
$$
q(a) = \mathbb{E}[R_t \ |\ A_t = a]
$$

$q(a)$는 실제의 그것이고, $Q_t(a)$는 우리가 추정하는 값이다.
간단하게 다음과 같이 추정할 수 있다.
$$
Q_t(a)\ =\ {\sum_{n=1}^t\ I(A_n = a)\ R_n \over \sum_{n=1}^t I(A_n = a)}
$$

$I$는 indicator function으로 True일 때 1이고, False일 때 0이다.
위의 평균 추정식에서 분모가 되는 걸 action에 대한 count $N_t(a)$라고 한다.

추정값 $Q_t(a)$는 다음과 같이 업데이트될 수 있다.

$$\begin{align}
Q_t(A_t)\ & =\ Q_{t-1}(A_t)\ +\ \alpha_t(R_t\ - Q_{t-1}(A_t)) \\
\forall\ a\ \neq\ A_t\ :\ Q_t(a)\ & =\ Q_{t-1}(a) \\\\

with\quad \alpha_t\ =\ {1 \over N_t(A_t)}\quad & and \quad N_t(A_t)\ =\ N_{t-1}(A_t)\ +\ 1
\end{align}$$

step size $\alpha$는 단순히 이렇게 카운트로 나눠주기 보다는 tracking 같은 방법도 있다고 한다.

### Regret

optimal value $v_*$는 당연하게 optimal action에 대한 q(a)이고 여기서 regret $\Delta_a$은 $v_*\ - \ q(a)$이다.
total regret $L_t$는 다음과 같다.
$$
L_t\ = \sum_{n=1}^t\ v_* - q(A_n)\ = \sum_{n=1}^t\ \Delta_{A_n}
$$

cumulative reward를 최대로 하는 것은 위의 total regret을 최소화하는 것과 같다.


## Greedy Algorithm

가장 간단한 policy 중 하나로 매번 $Q_t(a)$를 최대로 만드는 $a$를 선택하는 것이다.
하지만 위에서 언급했듯이 local optimal에 갇힐 수 있다.

## $\epsilon$-Greedy Algorithm

적당히 작은 $\epsilon$을 정해서 다음과 같이 랜덤하게 액션을 하는 것이다.
$$
\pi_t(a)\ =\ 
\begin{cases}
(1-\epsilon)\ + \epsilon/|\mathbb{A}| & \text{if}\quad Q_t(a)\ = \max_b\ Q_t(b) \\
\epsilon/|\mathbb{A}| & \text{otherwise}
\end{cases}
$$

하지만 계속해서 exploration을 한다는 문제가 있다.

근데 그러면 decaying $\epsilon$을 사용하면 되지 않을까? 

## Policy gradients

value를 학습하는 것 대신 policy만을 직접적으로 학습하는 방법이다.
예시로, action preferences $H_t$라는 policy parameter가 있을 때 다음과 같이 policy를 정의할 수 있다.
$$
\pi(a)\ =\ {e^{H_t(a)} \over \sum_b e^{H_t(b)}} 
$$


To be updated
