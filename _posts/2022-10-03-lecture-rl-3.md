---
title: DeepMind x UCL RL Lecture Series - 3. MDPs and Dynamic Programming
date: 2022-10-03  22:57:00 +0900
categories: [Lectures]
tags: [ml, rl, mdp, ]
math: true
---

# Exploration & Control

- [Video](https://www.youtube.com/watch?v=zSOMeug_i_M&list=PLqYmG7hTraZDVH599EItlEWsUOsJbAodm&index=3&ab_channel=DeepMind)
- [Slides](https://storage.googleapis.com/deepmind-media/UCL%20x%20DeepMind%202021/Lecture%203%20-%20MDPs%20and%20Dynamic%20Programming.pdf)


## Markov Decision Process

이전의 강의 내용들을 다시 짧게 요약하자면, Markov Decision Process $(\mathbb{S}, \mathbb{A}, p, r, \gamma)$는 다음과 같이 정리할 수 있다.

- $\mathbb{S}$는 가능한 state의 집합이다.
- $\mathbb{A}$는 가능한 action의 집합이다.
- $p(s^\prime|s,a)$는 state $s$에서 action $a$를 했을 때 state $s^\prime$으로 갈 확률이다.
- State $s$와 action $a$를 했을 때 얻어지는 reward $r$은 $\mathbb{E}[R|s,a]$이다.
- Discoutn factor $\gamma$는 0과 1사이의 값을 가지고, 커질수록 나중의 리워드를 높게 평가한다는 뜻이다.  

Markov property는 이전의 강의에서도 나왔듯이 MDP는 $S_t$에 우리에게 필요한 모든 정보가 있어 history를 저장하지 않아도 되는 decision process이다. 그리고 이전의 강의들의 RL 문제들도 MDP로 형식화할 수 있다. 당장 이전의 bandits의 경우엔 하나의 state를 갖는 MDP이다. 이 history에 무관하다는 Markov property를 식으로 표현하면 다음과 같다.

$$
P(S_{t+1}\ =\ s^\prime | S_t\ = s)\ =\ p(S_{t+1}\ =\ s^\prime| h_{t-1}, S_t\ =\ s)
$$

## Return

return은 reward들의 합이다. 단순히 합을 해줄 수도 있지만, 위에서 설명한 discount facotr를 적용한다던지 평균을 적용할 수도 있다.

### Discounted Return

이 때 discount factor를 적용한 return에 대해서 좀 더 설명을 하신다. 1보다 작은 $\gamma$에 대해서 지금 현재의 reward가 나중의 reward보다 중요하다는 뜻이다. 0에 가까워질수록 당장의 reward를 더 가치있게 평가하는 것이고 1에 가까워질수록 더 많은 스텝을 고려하는 것이다.

수학적으로도 discount factor를 적용한다면 cyclic Markov process에서 무한의 값을 가지는 return을 방지할 수 있다.

## Policy and Value Function

### Policy

policy란 state $s$에서 가능한 action들에 대한 확률 매핑을 의미하고, deterministic policy에서는 $a_t\ =\ \pi(s_t)$로 나타내기도 한다.

### Value Function

value function과 action value는 다음과 같이 정의할 수 있다.

$$\begin{aligned}
v_\pi(s)\ & =\ \mathbb{E}[G_t\ |\ S_t\ =\ s, \pi] \\
q_\pi(s, a)\ & =\ \mathbb{E}[G_t\ |\ S_t\ =\ s, A_t\ =\ a, \pi]
\end{aligned}$$

위 두 식으로부터 value function을 다시 정리할 수 있다.

$$
v_\pi(s)\ =\ \sum_a \pi(a\ |\ s)q_\pi(s, a)\ =\ \mathbb{E}[q_\pi(S_t, A_t)\ |\ S_t\ =\ s, \pi]
$$

### Optimal Value Function

Optimal state-value function $v^*(s)$이란 모든 policy에 대해 최대의 value를 갖도록 하는 함수이고, optimal action-value function $q^*(s, a)$이란 모든 policy에 대해 최대의 action value를 갖도록 하는 함수이다. 이 optimal들을 찾는 것이 MDP를 푸는 것이다.

### Optimal Policy

Optimal policy란 모든 state에 대해 다른 policy보다 더 좋은 value를 갖도록 하는 policy이다. Optimal policy는 optimal value function과 optimal action value function을 가능하게 한다.


## Bellman Equation

### Bellman Expectation Equation

value function을 재귀적으로 나타내보면 다음과 같다.

$$\begin{aligned}
v_{\pi}(s)\ 
& =\ \mathbb{E}[R_{t+1} + \gamma G_{t+1}\ |\ S_t\ =\ s, A_t \sim \pi(s)] \\
& =\ \mathbb{E}[R_{t+1} + \gamma v_{\pi}(S_{t+1})\ |\ S_t\ =\ s, A_t \sim \pi(s)] \\
& =\ \sum_a \pi(a\ |\ s)\sum_r\sum_{s^\prime}p(r,s^\prime\ |\ s,a)(r\ +\ \gamma v_\pi(s^\prime))
\end{aligned}$$

action value는 다음과 같고,

$$\begin{aligned}
q_\pi(s, a)\ =\ \mathbb{E}[R_{t+1} + \gamma v_\pi (S_{t+1})\ |\ S_t\ =\ s, A_t\ =\ a] \\
\end{aligned}$$

위에서 $v_\pi(s)\ =\ \sum_a \pi(a\ |\ s)q_\pi(s, a)$였기 때문에 다시 정리할 수 있다.

$$\begin{aligned}
q_{\pi}(s, a)\ =\ \sum_r\sum_{s^\prime}p(r,s^\prime\ |\ s,a)(r\ +\ \gamma \sum_{a^\prime} \pi(a^\prime\ |\ s^\prime)q_\pi(s^\prime, a^\prime))
\end{aligned}$$

$$\begin{aligned}
v_\pi(s)\ & =\ \sum_a \pi(s, a) \Biggl[r(s,a)\ +\ \gamma \sum_{s^\prime}p(s^\prime\ |\ s,a) v_\pi(s^\prime)\Biggr]\\
q_{\pi}(s, a)\ & =\ r(s,a)\ + \gamma \sum_{s^\prime}p(s^\prime|a, s) \sum_{a^\prime} \pi(a^\prime\ |\ s^\prime)q_\pi(s^\prime, a^\prime))
\end{aligned}$$
MDP에서 value function과 action value는 위의 두 식대로 계산되고, 이를 Bellman Expectation Equation이라고 한다.

### Bellman Optimality Equation

optimal에 도달했을 때에는 value가 최대로 되게 하는 action에 대해 $\pi(s, a)$가 1이기 때문에 다음과 같다.

$$\begin{aligned}
v^*(s)\ & =\ \max_a \Biggl[r(s,a)\ +\ \gamma \sum_{s^\prime}p(s^\prime\ |\ s,a) v_\pi(s^\prime)\Biggr]\\
q^*(s, a)\ & =\ r(s,a)\ + \gamma \sum_{s^\prime}p(s^\prime|a, s) \max_{a^\prime \in \mathbb{A}}q^*(s^\prime, a^\prime))
\end{aligned}$$


### Bellman Equation in Matrix Form

그럼 environment의 정보를 충분히 안다는 가정하에 Bellman equation을 통해 state의 value를 모두 구할 수 있다. $v$를 state의 value 벡터, $r$을 reward 벡터, $P$를 state에서 다른 state로 전이될 확률 행렬이라고 했을 때 value를 구하는 식은 다음과 같다.

$$
v\ =\ (I - \gamma P^\pi)^{-1}r^\pi
$$

이 식의 복잡도는 $O(|S|^3)$이고 이렇게 value를 얻어 내는 것은 아주 작은 문제에서나 가능하다는 것을 알 수 있다.
그래서 더 큰 문제들에 대해서는 DP, Monte-Carlo evaluation, Temporal-Difference learning 같은 iterative method를 이용한다.

## Dynamic Programming

### Policy Evaluation

$$
v_\pi(s)\ =\ \mathbb{E}[R_{t+1}\ +\ \gamma v_\pi(S_{t+1})\ |\ s, \pi]
$$

value를 구하는 식에서 알 수 있듯이 다음 state에 대한 value를 모른다면 현재의 value를 구할 수 없다. 그래서 모든 value를 0으로 초기화하고 현재 알고있는 것만 계속 업데이트 해나가다 점점 수렴할 때까지 그 과정을 반복하는 것을 policy evaluation이라고 한다.

### Policy Improvement

policy evaluation이 끝나고 각 state에서 최대의 value를 가지는 state로 전이하도록 greedy하게 policy를 수정할 수도 있다. 실제로는 greedy만으로는 문제를 풀 수 있는 경우가 잘 없지만 이렇게 greedy로 action을 취하도록 policy를 수정하는 것을 policy improvement라고 한다.

### Policy Iteration

Policy evaluation을 진행하고 업데이트된 value에 따라 policy improvement를 진행하는 과정을 반복하는 것이 policy iteration이다.

여기서 policy evaluation이 converge하기 전에 policy improvement를 할 수 없을까? 강의에서 든 예시인 gridworld의 경우엔 policy evaluation에서 $k\ =\ 3$ 로도 충분했다. 

### Value Iteration 

Policy iteration을 극단적으로 $k\ =\ 1$로 줄이면 value iteration이다.

## Asynchronous Dynamic Programming

위의 방법들은 모두 synchronous DP 알고리즘이다. asynchronous하게 할 수 있다면 더 빠르게 solution을 찾을 수 있다.

### In-Place Dynamic Programming

synchronous에서는 value function의 복사본을 가지고 예전 value의 값으로 새로운 value를 구하는 것이었다면, in-place value iteration에서는 하나의 value function만을 가지고 현재 사용하고 있는 value function에다 바로 업데이트를 하는 것이다. 음 이렇게 하면 state의 순서에 따라 value function이 다르게 업데이트 되지 않나?

### Real-Time Dynamic Programming

Agent와 관계있는 state들만 업데이트하는 방식이다.

### Sample Backups

일반적으로 DP에서는 full-width backups 즉 모든 state에 대해 iteration을 진행한다. Full backup은 상당히 코스트가 많이 드는 방법이기 때문에 sample backup을 진행하기도 한다.