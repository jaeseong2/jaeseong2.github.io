---
title: DeepMind x UCL RL Lecture Series - 3. MDPs and Dynamic Programming
date: 2022-10-03  22:57:00 +0900
categories: [Lectures]
tags: [ml, rl, mdp, ]
math: true
---

# Exploration & Control

- [Video](https://www.youtube.com/watch?v=zSOMeug_i_M&list=PLqYmG7hTraZDVH599EItlEWsUOsJbAodm&index=3&ab_channel=DeepMind)
- [Slides](https://storage.googleapis.com/deepmind-media/UCL%20x%20DeepMind%202021/Lecture%203%20-%20MDPs%20and%20Dynamic%20Programming.pdf)


## Markov Decision Process

이전의 강의 내용들을 다시 짧게 요약하자면, Markov Decision Process $(\mathbb{S}, \mathbb{A}, p, r, \gamma)$는 다음과 같이 정리할 수 있다.

- $\mathbb{S}$는 가능한 state의 집합이다.
- $\mathbb{A}$는 가능한 action의 집합이다.
- $p(s^\prime|s,a)$는 state $s$에서 action $a$를 했을 때 state $s^\prime$으로 갈 확률이다.
- State $s$와 action $a$를 했을 때 얻어지는 reward $r$은 $\mathbb{E}[R|s,a]$이다.
- Discoutn factor $\gamma$는 0과 1사이의 값을 가지고, 커질수록 나중의 리워드를 높게 평가한다는 뜻이다.  

Markov property는 이전의 강의에서도 나왔듯이 MDP는 $S_t$에 우리에게 필요한 모든 정보가 있어 history를 저장하지 않아도 되는 decision process이다. 그리고 이전의 강의들의 RL 문제들도 MDP로 형식화할 수 있다. 당장 이전의 bandits의 경우엔 하나의 state를 갖는 MDP이다. 이 history에 무관하다는 Markov property를 식으로 표현하면 다음과 같다.

$$
P(S_{t+1}\ =\ s^\prime | S_t\ = s)\ =\ p(S_{t+1}\ =\ s^\prime| h_{t-1}, S_t\ =\ s)
$$

## Return

return은 reward들의 합이다. 단순히 합을 해줄 수도 있지만, 위에서 설명한 discount facotr를 적용한다던지 평균을 적용할 수도 있다.

### Discounted Return

이 때 discount factor를 적용한 return에 대해서 좀 더 설명을 하신다. 1보다 작은 $\gamma$에 대해서 지금 현재의 reward가 나중의 reward보다 중요하다는 뜻이다. 0에 가까워질수록 당장의 reward를 더 가치있게 평가하는 것이고 1에 가까워질수록 더 많은 스텝을 고려하는 것이다.

수학적으로도 discount factor를 적용한다면 cyclic Markov process에서 무한의 값을 가지는 return을 방지할 수 있다.

## Policy and Value Function

### Policy

policy란 state $s$에서 가능한 action들에 대한 확률 매핑을 의미하고, deterministic policy에서는 $a_t\ =\ \pi(s_t)$로 나타내기도 한다.

### Value Function

value function과 action value는 다음과 같이 정의할 수 있다.

$$\begin{aligned}
v_\pi(s)\ & =\ \mathbb{E}[G_t\ |\ S_t\ =\ s, \pi] \\
q_\pi(s, a)\ & =\ \mathbb{E}[G_t\ |\ S_t\ =\ s, A_t\ =\ a, \pi]
\end{aligned}$$

위 두 식으로부터 value function을 다시 정리할 수 있다.

$$
v_\pi(s)\ =\ \sum_a \pi(a\ |\ s)q_\pi(s, a)\ =\ \mathbb{E}[q_\pi(S_t, A_t)\ |\ S_t\ =\ s, \pi]
$$

### Optimal Value Function

Optimal state-value function $v^*(s)$이란 모든 policy에 대해 최대의 value를 갖도록 하는 함수이고, optimal action-value function $q^*(s, a)$이란 모든 policy에 대해 최대의 action value를 갖도록 하는 함수이다. 이 optimal들을 찾는 것이 MDP를 푸는 것이다.

### Optimal Policy

Optimal policy란 모든 state에 대해 다른 policy보다 더 좋은 value를 갖도록 하는 policy이다. Optimal policy는 optimal value function과 optimal action value function을 가능하게 한다.


## Bellman Equation


### Bellman Equation in Matrix Form

위에서 Bellman optimality equation을 푸는 것은 아주 작은 문제에서나 


## Dynamic Programming

### Policy evaluation


### Policy improvement

