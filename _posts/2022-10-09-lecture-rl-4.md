---
title: DeepMind x UCL RL Lecture Series - 4. Theoretical Fund. of Dynamic Programming Algorithms
date: 2022-10-09  22:57:00 +0900
categories: [Lectures, RL]
tags: [ml, rl, mdp, bellman operators]
math: true
---

# Theoretical Fund. of Dynamic Programming Algorithms

- [Video](https://www.youtube.com/watch?v=XpbLq7rIJAA&list=PLqYmG7hTraZDVH599EItlEWsUOsJbAodm&index=4&ab_channel=DeepMind)
- [Slides](https://storage.googleapis.com/deepmind-media/UCL%20x%20DeepMind%202021/Lecture%204%20-%20Theoretical%20Fundamentals%20of%20DP%20Algorithms.pdf)


이번 강의에서는 지난 강의에서 나왔던 bellman equation에 대해 더 수학적으로 접근하고, value iteration과 policy iteration이 수렴하는지에 대해 수학적으로 증명한다.

강의를 본격적으로 시작하시기 전, 베이스로 normed vector space, contraction mapping, fixed point, banach fixed point theorem에 대해 설명을 하시고 시작을 하신다.


## Bellman Optimality Operator

어떤 real-value function $\mathbb{V}$에 대해 Bellman optimality operator $T_\mathbb{V}^* : \mathbb{V} \rightarrow \mathbb{V}$가 다음과 같은 식을 만족한다.

$$
(T_\mathbb{V}^*f)(s)\ =\ \max_a \Biggl [r(s,a)\ +\ \gamma \sum_{s^\prime}p(s^\prime\ |\ a, s)f(s^\prime) \Biggr]
$$

### Properties of the Bellman Operator

이 Bellman operator는 value function이 optimal에 도달했을 때 더 이상 변하지 않기 때문에 fixed point를 가지고, $\gamma$-contraction이고, monotonic이다.

$\gamma$-contraction에 대한 증명은 다음과 같다.

![gamma-contraction proof](/assets/images/gamma-contraction_proof.PNG)

monotonic에 대한 증명은 다음과 같다.

![monotonic proof](/assets/images/monotonic_proof.PNG)

### Value Iteration through the lens of the Bellman Operator

위의 bellman operator의 특성을 이용해 value iteration이 왜 converge하는지 증명할 수 있다. Banach Fixed Point Theorem과 동일하게 적용하여 증명한다.

![monotonic proof](/assets/images/value_iteration_proof.PNG)

동일하게 policy iteration에도 적용할 수 있다.


## Approximate Dynamic Programming

위에서 우리는 MDP에 대한 정확한 지식과 정확하고 완벽한 value function을 가지고 있다고 가정했다. 하지만 실제로는 그렇지 않다. 우리는 MDP에 대해서 정확하게(environment)에 대해서 정확하게 알지도 못하고, value function을 정확하게 나타낼 수도 없다.

그래서 우리는 value iteration에 다음 스텝의 value function을 approximate한 value function으로 갱신한다. 그래서 일반적인 경우에는 수렴하지 않는다.

