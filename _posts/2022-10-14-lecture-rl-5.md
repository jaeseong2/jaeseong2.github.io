---
title: DeepMind x UCL RL Lecture Series - 5. Model-free Prediction
date: 2022-10-14 22:57:00 +0900
categories: [Lectures, RL]
tags: [ml, rl, monte carlo, tempral difference, multi-step td, eligibility traces]
math: true
---

# Model-free Prediction

- [Video](https://www.youtube.com/watch?v=eaWfWoVUTEw&list=PLqYmG7hTraZDVH599EItlEWsUOsJbAodm&index=5&ab_channel=DeepMind)
- [Slides](https://dpmd.ai/modelfreeprediction)


## Value Function Approximation

이전 강의까지 일반적으로 우리는 value function을 모든 state마다 그에 해당하는 value가 있는 lookup table으로 생각해왔다. 하지만 large MDP에서는 lookup table처럼 value function을 구성할 수 없다. 모든 state와 그에 맞는 value를 메모리에 모두 저장할 수 없기 때문이다. 그만한 메모리가 있다고 해도 fully observable하지 않을 수도 있고, 모든 state를 거쳐야하기 때문에 학습의 진행도가 매우 느리다.

그래서 value function을 function approximation을 통해 근사한다. 그래서 이전에 경험해보지 못했던 state에 대해서도 value를 찾을 수 있도록 한다.


## Agent State Update

Large MDP에서 environment state는 모두 관측할 수 없기 때문에 agent state를 이용한다. 이때 agent state의 파라메터로는 일반적으로 벡터를 이용할 수 있고, 이 벡터들을 단순히 현재의 관측이라고 생각하는 것이다.


## Monte Carlo Algorithms

모델 없이 sample만을 가지고 학습하는데 이용할 수 있다. Monte Carlo Algorithms은 에피소드에서 끝까지 샘플링을 진행하여 그것을 학습하는데 이용하는 알고리즘이다.

### Monte Carlo Policy Evaluation

Bandits을 생략하고 바로 sequential decision problem에 대해서 보자. Policy $\pi$에 대해 $v_\pi$를 학습하기 위해 return을 사용할 수 있다. Return에 대해 다시 정리하면, total discounted reward로 에피소드가 끝날 때까지의 reward를 discount factor를 적용하여 더한 것이다. 우리는 sample된 return의 평균값을 기댓값 대신에 쓸 수도 있다. 이것을 Monte Carlo policy evalution이라고 한다.


### Disadvantages of Monte Carlo Learning

블랙잭을 에피소드 수에 따라 샘플링한 값으로 예상한 value function이 어떻게 근사하는지에 대해 예시로 보여주신다. 잘 되는 듯보이나 MC에는 에피소드가 길어지면 학습이 느려진다는 문제가 있다. Return을 샘플하기 위해서는 에피소드가 끝날 때까지 기다려야 하기 때문이다. 또한 여러 스텝이 쌓이는 것이기 때문에 리턴은 큰 variance를 가질 수 있다.


## Temporal-Difference Learning

위의 MC의 단점을 해결할 수 있는 알고리즘이다. 이전 강의에서 Bellman equation은 다음과 같았다.

$$\begin{aligned}
v_{\pi}(s)\ 
& =\ \mathbb{E}[R_{t+1} + \gamma G_{t+1}\ |\ S_t\ =\ s, A_t \sim \pi(s)] \\
& =\ \mathbb{E}[R_{t+1} + \gamma v_{\pi}(S_{t+1})\ |\ S_t\ =\ s, A_t \sim \pi(s)]
\end{aligned}$$

그렇다면 $R_{t+1}\ +\ \gamma v_{\pi}(S_{t+1})$를 추정하여 value function을 업데이트하는 것이 TD learning이다.

MC와 TD의 업데이트를 식으로 비교해보면 다음과 같다.

$$
v_{n+1}(S_t)\ =\ v_n(S_t)\ +\ \alpha(G_t\ -\ v_n(S_t))
$$

$$
v_{n+1}(S_t)\ \leftarrow\ v_n(S_t)\ +\ \alpha(R_{t+1}\ +\ \gamma v_{\pi}(S_{t+1})\ -\ v_n(S_t))
$$

$R_{t+1}\ +\ \gamma v_{\pi}(S_{t+1})$를 target이라고 하고,
$R_{t+1}\ +\ \gamma v_{\pi}(S_{t+1})\ -\ v_n(S_t)$를 TD error라고 한다.


### Bootstrapping and Sampling

Bootstrapping은 현재 추정한 값으로 다음에 추정할 값을 업데이트 하는 것이다. 통계에서는 데이터 셋에서 다시 샘플링하는 것으로 알고 있었어서 비슷하지만 다른 느낌이었다. Sampling은 앞에서 썼듯이 샘플링을 통해 기댓값을 예측하는 것이다.


### SARSA

위에서는 value를 예측하는 것이었지만 action value에 대해서도 TD를 적용할 수 있다. 이를 SARSA라고 한다. Value 대신에 action value를 예측하는 q가 들어가면 된다.

$$
q_{t+1}(S_t, A_t)\ \leftarrow\ q_t(S_t, A_t)\ +\ \alpha(R_{t+1}\ +\ \gamma q_t(S_{t+1}, A_{t+1})\ -\ q_t(S_t, A_t))
$$


### Properties of TD Learning

TD는 MC와 마찬가지로 model-free이며, bootstrapping을 하기 때문에 에피소드가 끝나지 않아도 학습을 할 수 있다. 또한 에피소드 내에서도 학습을 진행할 수 있다.


## Comparing MC and TD

### Adv and Dis of MC vs. TD

TD는 최종 결과를 알기 전에 학습할 수 있다. 그래서 매 스텝 학습을 진행할 수 있으나 MC는 에피소드가 끝날 때까지 기다려야 한다. 

TD는 시간적 범위에 무관하다. 왜냐하면 매번의 전이마다 학습을 진행할 수 있기 때문이다. 반면에, MC는 모든 상태나 예측을 에피소드가 끝날 때까지 들고 있어야한다. 에피소드가 너무 길다면 학습하기 어려울 수 있다.


### Bias/Variance Trade-Off

MC의 경우에 정확하게 Return으로만 계산하기 때문에 unbiased된 추정값을 얻을 수 있다. 하지만 TD는 예측을 통해 값을 얻기 때문에 biased된 추정값을 얻는다. 하지만 TD는 MC에 비해 상대적으로 작은 vairance를 갖는다. Return은 많은 스텝에 걸쳐서 얻어지지만 TD target은 한 스텝에 대하여 구해지는 것이기 때문이다.


## Batch MC and TD

![batch_mc_td](/assets/images/batch_mc_td.PNG)

위 같이 8개의 에피소드 데이터가 있다고 했을 때 MC와 TD에서 $v(A), v(B)$가 어떻게 다른지 알아보자.

먼저, MC의 경우엔 $v(A), v(B)$는각각 0, 0.75가 될 것이다. 관측했던 return대로면 value가 저렇게 된다. 
하지만 TD의 경우엔 bootstrap을 진행하고, 데이터 상으로 state A에서 state B로 가는 확률이 1이기 때문에 $v(A), v(B)$를 모두 0.75로 예측한다.

이렇게 유한한 데이터 셋에서 MC와 TD의 결과는 다를 수 있다.


## Multi-Step TD

TD는 추정값으로 업데이트를 하는 것이기 때문에 부정확할 수 있다. 또한 MC에 비해 상대적으로 정보가 전파되는 속도가 느릴 수 있다. 그렇다면 TD와 MC의 중간에서 둘 모두의 장점을 갖는 알고리즘은 없을까.

그게 Multi-Step TD이고, 이전의 TD에서는 바로 다음 스텝의 value까지만 봤다면 multi step에서는 더 많은 스텝을 나아간 state까지의 value를 보는 것이다. n-step return을 식으로 표현하면 다음과 같다.

$$
G_t^{(n)}\ =\ R_{t+1}\ +\ \gamma R_{t+2}\ +\ ...\ +\ \gamma^{n-1}R_{t+n} + \gamma^nv(S_{t+n})
$$


### Mixed Multi-Step Returns

위의 n-step return처럼 딱 몇 스텝까지를 잘라서 사용할 수도 있지만 return 자체에 discount factor처럼 0과 1사이의 값을 곱해서 스텝이 지날수록 영향이 약하게 만들 수도 있다.

$$
G_t^\lambda\ =\ R_{t+1}\ +\ \gamma \Bigl( (1-\lambda)v(S_{t+1})\ +\ \lambda G_{t+1}^\lambda \Bigr)
$$

이걸 일반식으로 표현하면,

$$
G_t^\lambda\ =\ \sum_{n=1}^\infty (1-\lambda)\lambda^{n-1} G_{t}^{(n)}
$$

람다가 0일 때 TD가 되고, 람다가 1일때 MC가 된다.


### Benefits of multi-step returns

Multi-step return은 TD와 MC의 장점을 모두 가지고 있다. 하지만 동시에 bootstrapping의 bias와 MC의 vairance 이슈를 갖는다. 일반적으로 중간의 $n\ =\ 10, \lambda\ =\ 0.9$같은 값이 좋다고 하신다.


## Eligibility Traces

Multi-step return에서도 역시 모든 에피소드가 끝나야 업데이트를 할 수 있다는 단점이 있다. TD처럼 즉시 갱신하기 위한 방법이 Eligibility Trace이다.

MC error를 TD error $\delta_t$의 sum으로 바꾸어서 업데이트 하는 것이다.

$$\begin{aligned}
G_t\ -\ v(S_t)\ & =\ R_{t+1}\ +\ \lambda G_{t+1}\ -\ v(S_t) \\
& =\ R_{t+1}\ +\ \lambda v(S_{t+1})\ -\ v(S_t) + \lambda (G_{t+1}\ -\ v(S_{t+1})) \\
& =\ \delta_t\ +\ \lambda (G_{t+1}\ -\ v(S_{t+1})) \\
& =\ \delta_t\ +\ \lambda \delta_t\ +\ \lambda^2 (G_{t+2}\ -\ v(S_{t+2})) \\
& =\ \sum_{k=t}^T \lambda^{k-t} \delta_k
\end{aligned}$$

지금 인사배치 PPO 알고리즘에서 학습에 사용하는 loss도 위와 같은 방식으로 계산하는 것이다.


### Mixing multi-step returns & traces

위의 eligibility tracef를 mixed multi-step과 합하여 나타낼 수 있다.

먼저, mixed multi-step return은 다음과 같았다.

$$
G_t^\lambda\ =\ R_{t+1}\ +\ \gamma \Bigl( (1-\lambda)v(S_{t+1})\ +\ \lambda G_{t+1}^\lambda \Bigr)
$$

Trace로 나타내면, 다음과 같고 이를 accumulating trace라고 한다.

$$
G_t^\lambda\ =\ \sum_{k=0}^{T-t} \lambda^k \gamma^k \delta_{t+k}
$$
