---
title: DeepMind x UCL RL Lecture Series - 6. Model-free Control
date: 2022-10-15 22:57:00 +0900
categories: [Lectures, RL]
tags: [ml, rl, ]
math: true
---

# Model-free Control

- [Video](https://www.youtube.com/watch?v=t9uf9cuogBo&list=PLqYmG7hTraZDVH599EItlEWsUOsJbAodm&index=6&ab_channel=DeepMind)
- [Slides](https://dpmd.ai/modelfreecontrol)


## Monte Carlo Control

### Model-Free Policy Iteration Using Action-Value Function

단순하게 greedy로 $q(s, a)$를 가장 크게 만드는 액션을 선택하도록 policy를 선택할 수 있다. 하지만 greedy는 exploration을 하지 않기 때문에 policy improvement를 하지 못한다.
그래서 대신에 $\epsilon$-greedy같은 방법을 이용할 수 있다.

### GLIE

Greedy in the Limit with Infinite Exploration으로 모든 state-action 쌍을 무한히 탐험하다보면 policy는 greedy policy로 수렴할 것이라는 것이다. 예전에 $\epsilon$-greedy가 처음 나온 강의를 정리할 때 했던 생각과 비슷한데, 
예시로 $\epsilon\ =\ {1 \over k}$를 사용하는 입실론 그리디를 예시로 드셨다.


## Temporal-Difference Learning For Control

### MC vs. TD Control

TD는 MC에 비해 다음과 같은 장점이 있다.

- Lower variance
- Online
- Can learn from incomplete sequences

그래서 대신에 $q(s,a)$에 TD를 적용할 수 있다.

### SARSA

![SARSA](/assets/images/SARSA.PNG)

그렇게 적용한 것이 지난 강의에서 언급했던 SARSA이다. state, action, reward를 나타낸 위 그림에서 순서대로 읽으면 SARSA라서 SARSA라고 붙여졌다고 하신다 ㅋㅋㅋ..

위의 MC와 동일하게 policy evaluation 단계에서 SARSA를 사용하고, improvement에서는 입실론 그리디를 사용할 수 있다.


## Off-policy TD and Q-learning

### Dynamic Programming

이전에 DP에서 다음과 같은 value와 action value에 관한 알고리즘을 배웠었다.

$$\begin{aligned}
v_{k+1}(s)\ & =\ \mathbb{E}[R_{t+1} + \gamma v_k(S_{t+1})\ |\ S_t\ =\ s, A_t \sim \pi(S_t)] \\
v_{k+1}(s)\ & =\ \max_a \mathbb{E}[R_{t+1} + \gamma v_k(S_{t+1})\ |\ S_t\ =\ s, A_t\ =\ a] \\
q_{k+1}(s, a)\ & =\ \mathbb{E}[R_{t+1} + \gamma q_k(S_{t+1}, A_{t+1})\ |\ S_t\ =\ s, A_t\ =\ a] \\
q_{k+1}(s, a)\ & =\ \mathbb{E}[R_{t+1} + \gamma \max_{a^\prime} q_k(S_{t+1}, a^\prime)\ |\ S_t\ =\ s, A_t\ =\ a] \\
\end{aligned}$$

### TD learning

위의 DP에서 사용한 식들을 model-free TD algorithms에 맞게 각각 적어보면 다음과 같다.

$$\begin{aligned}
v_{t+1}(s)\ & =\ v_t(S_t)\ +\ \alpha_t(R_{t+1}\ +\ \gamma v_t(S_{t+1})\ -\ v_t(S_t)) \\
q_{t+1}(s, a)\ & =\ q_t(S_t, A_t)\ +\ \alpha_t(R_{t+1}\ +\ \gamma q_t(S_{t+1}, A_{t+1})\ -\ q_t(S_t, A_t)) \\
q_{t+1}(s, a)\ & =\ q_t(S_t, A_t)\ +\ \alpha_t(R_{t+1}\ +\ \gamma \max_{\alpha^\prime} q_t(S_{t+1}, a^\prime)\ -\ q_t(S_t, A_t)) \\
\end{aligned}$$

위에서부터 TD, SARSA, Q-learning이다. 위의 DP에서는 식이 4개인데 왜 model-free TD 알고리즘에선 3개밖에 없을까?

그건 두번째 value iteration 알고리즘에서 기댓값이 max 내에 있기 때문이다. 나머지 알고리즘들은 모두 기댓값에 대한 알고리즘이기 때문에 간단하게 샘플링으로도 그걸 추정할 수 있지만, 기댓값의 최댓값이라면 단순하게 샘플링해서 구할 수 있는 값이 아니기 때문이다.

### On and Off-Policy Learning

On-policy learning이란 action을 생성하는 behavior policy와 value를 계산하는 target policy를 같게 사용하여 학습하는 것이고, off-policy learning은 behavior policy와 target policy를 다르게 갖고 있는 것이다.

### Off-Policy Learning

Value를 계산하는 policy와 실제로 action을 선택하는 policy를 다르게 가져가는 off-policy는 왜 중요할까? 

- Learn from observing humans or other agents
- Re-use experience from old policies
- Learn about multiple policies while following one policy
- Learn about greedy policy while following exploratory policy

위의 점 중에서 탐험을 하면서 다양한 policy를 탐험하며 optimal을 찾을 수 있다는 점이 가장 크게 느껴졌다.

Q-learning에서는 greedy policy를 target policy로 사용한다. Greedy는 탐험을 충분하게 하지 못한다는 문제가 있다. 하지만 꼭 greedy하게 action을 선택하지 않음으로써 optimal value function에 수렴할 수 있다.

그리고서는 cliff walking example을 예시로 설명을 해주신다. 나오는 그래프에는 당장 SARSA가 더 높은 reward를 가지고 있지만 최종적으로는 Q-learning이 더 optimal한 path를 찾는다.


## Overestimation in Q-learning

### Q-learning overestimation

Classical Q-learning에서 잠재적 이슈가 있다. 바로 select와 evaluate에 같은 action value를 사용한다는 것이다. 식으로 다시 표현하면 다음과 같다.

$$
\max_a q_t(S_{t+1}, a)\ =\ q_t(S_{t+1}, \argmax_a q_t(S_{t+1}, a))
$$

Target을 계산하는 과정에서 a를 선택할 때 action value를 가장 크게 갖는 a를 select해서 그 a에 대해 똑같은 action value function을 사용해서 evaluate한다는 말이다. 저 action value가 optimal이라면 문제가 되지 않는다. 하지만 우리는 학습을 시작할 때 아무것도 모르는 채로 시작하게 된다. 그렇다면 overestimated value에 대해서는 더 많이 선택되게 되고, underestimated value에 대해서는 더 적게 선택되게 한다. 그래서 bias가 높아지게 된다.

### Double Q-learning

위의 bias 문제를 해결하기 위해서는 select와 evaluate에 사용하는 action value function을 다르게 사용하면 된다. Q-learning에 대해 그렇게 두 개의 function을 사용하는 것을 Double Q-learning이라고 한다.

$$
R_{t+1}\ +\ \gamma q_t^\prime(S_{t+1}, \argmax_a q_t(S_{t+1}, a)) \\
R_{t+1}\ +\ \gamma q_t(S_{t+1}, \argmax_a q_t^\prime(S_{t+1}, a))
$$

### Double learning

Q-learning에서 뿐만 아니라 다른 알고리즘으로도 확장될 수 있다. 예를 들어, SARSA에서 $\epsilon$-greedy를 사용한다면 이 역시도 overestimate될 수 있다. 왜냐하면 greedy라는 알고리즘 자체가 value와 policy의 연관성이 높은 알고리즘이기 때문이다. SARSA에 적용한다면, double SARSA가 된다.


## Off-Policy Learning: Importance Sampling Corrections

### Off-policy learning

위에서 Q-learning을 off-policy learning의 한 예시로 소개했지만 좀 더 일반화 할 수도 있다.

### Importance sampling corrections

예전에 학교에 있을 때 배운 importance sampling으로 두 개의 distirbution(policy)에서 expectation을 추정하는 것을 나타내보자.

$$\begin{aligned}
\mathbb{E}_{x \sim d}[f(x)]\ & =\ \sum d(x)f(x) \\
& =\ \sum d^\prime(x){d(x) \over d^\prime}f(x) \\
& =\ \mathbb{E}_{x \sim d^\prime} \Biggl[{d(x) \over d^\prime(x)}f(x) \Biggr]
\end{aligned}$$

직관적으로, $d^\prime$에서 빈도가 낮으면서
$d$에서 일반적인 이벤트에 대해서는 scale up해야 하고,
$d^\prime$에서 일반적이면서
$d$에서 빈도가 낮은 이벤트에 대해서는 scale down 해야 한다는 것을 알 수 있다.

이걸 off-policy learning에서 사용한 behavior policy $\mu$와
target policy $\pi$로 나타내보면 다음과 같다.

$$\begin{aligned}
\mathbb{E}[R_{t+1}\ |\ S_t\ =\ s, A_t \sim \pi]\ & =\ \sum_a \pi(a|s)r(s,a) \\
& =\ \sum \mu(a|s){\pi(a|s) \over \mu(a|s)}r(s,a) \\
& =\ \mathbb{E}\Biggl[{\pi(a|s) \over \mu(a|s)}R_{t+1}\ |\ S_t\ =\ s, A_t \sim \mu \Biggr]
\end{aligned}$$

### Importance Sampling for Off-Policy Monte-Carlo

MC에서는 return을 사용했기 때문에 그 연속된 과정을 모두 곱으로 나타내야 한다. 일련의 state, action, reward trajectory를 $\tau$라고 하자.

$$\begin{aligned}
{p(\tau_t|\pi) \over p(\tau_t|\mu)}G(\tau_t)\ & =\ {\pi(A_t|S_t)p(R_{t+1}, S_{t+1}|S_t,A_t)\pi(A_{t+1}|S_{t+1}) \cdots \over \mu(A_t|S_t)p(R_{t+1}, S_{t+1}|S_t,A_t)\mu(A_{t+1}|S_{t+1})\cdots} G_t\\
& =\ {\pi(A_t|S_t)\pi(A_{t+1}|S_{t+1}) \cdots \over \mu(A_t|S_t)\mu(A_{t+1}|S_{t+1})\cdots} G_t \\
\end{aligned}$$

### Importance Sampling for Off-Policy TD Updates

TD에서는 MC와 다르게 단일 importance sampling이다. 

$$
v(S_t)\ \leftarrow\ v(S_t)\ +\ \alpha \Bigl( {\pi(A_t|S_t) \over \mu(A_t|S_t)}(R_{t+1}\ +\ \gamma v(S_{t+1}) - v(S_t))\Bigr)
$$

Importance sampling에서도 MC가 TD보다 variance가 높다는 것을 확인할 수 있다.


### Expected SARSA

Action value에 대한 off policy learning을 생각해보자. Importance sampling은 필요없고, behavior policy에서 action을 선택하여 target policy에서 value를 estimate하여 갱신하면 된다.

$$
q(S_t, A_t)\ \leftarrow\ q(S_t, A_t)\ +\ \alpha \Bigl( R_{t+1}\ +\ \gamma \sum_a \pi(a|S_{t+1})q(S_{t+1}, a) - q(S_t, A_t)\Bigr)
$$

이걸 Expected SARSA라고 부르고, General Q-learning이라고 부르기도 한다. Q-learning은 target policy가 greedy인 케이스이다.